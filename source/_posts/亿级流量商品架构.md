---
title: 亿级流量架构
top_img: https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/moutain.png
cover: https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/moutain.png
date: 2023-02-10
description: 高并发架构
---



# 亿级流量架构

### 1. 小流量商品架构

![小流量架构-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%B0%8F%E6%B5%81%E9%87%8F%E6%9E%B6%E6%9E%84-%E5%AF%BC%E5%87%BA.png)

适用范围：

- 商品数量：1000

- 模板数量：1

- 静态页面：1000 * 机房（服务）数量

- 小米商城、华为商城等

#### 1.1 页面静态化

使用`FreeMarker`模板引擎。基于模板和数据源生成输出文本（HTML网页、配置文件）。

`pom` 引入：

```xml
<dependency>
    <groupId>org.freemarker</groupId>
    <artifactId>freemarker</artifactId>
    <version>2.3.23</version>
</dependency>
```

使用示例：

```java
public class FreeMarkTest {

    public static void main(String[] args) throws Exception {
        // 第一步：创建一个Configuration对象，直接new一个对象。构造方法的参数就是freemarker对于的版本号。
        Configuration configuration = new Configuration(Configuration.getVersion());
        // 第二步：设置模板文件所在的路径。
        configuration.setDirectoryForTemplateLoading(new File("G:\\tuling\\ftl"));
        // 第三步：设置模板文件使用的字符集。一般就是utf-8.
        configuration.setDefaultEncoding("utf-8");
        // 第四步：加载一个模板，创建一个模板对象。
        Template template = configuration.getTemplate("test.ftl");
        // 第五步：创建一个模板使用的数据集，可以是pojo也可以是map。一般是Map。
        Map dataModel = new HashMap<>();
        //向数据集中添加数据
        dataModel.put("hello", "monkey老师2021年6月6日21:21:08");
        // 第六步：创建一个Writer对象，一般创建一FileWriter对象，指定生成的文件名。
        Writer out = new FileWriter(new File("G:\\tuling\\ftl\\test.html"));
        // 第七步：调用模板对象的process方法输出文件。
        template.process(dataModel, out);
        // 第八步：关闭流。
        out.close();
    }
}
```

#### 1.2 架构方案的问题

##### 1.2.1 数据同步

数据新增分增量和全量数据，商品需要进行静态化。新增的商品静态化数据如何同步？

不同的应用部署在不同服务器甚至在不同的机房和国家。

1. ##### 通过网络同步的方式

   其中一台服务器静态话后，把文件同步到其它应用服务器上。比如 `linux` 的 `scp` 命令。

   方法可行，但问题较多，有多少个节点就需要同步多少份，等于是商品数量 * 服务器。

   不是最优。

2. ##### 定时任务

   可以在每个应用使用定时任务，分别去执行数据可需要静态化的数据即可。

   可以解决1中数据同步问题。因为所有的任务都是在本机运行，不需要数据同步。

   弊端是无法避免不同的服务器跑的数据不能重复，也就是 `A` 和 `B` 定时任务都跑了一份商品。（比较直观的就是上锁）

3. ##### 消息中间件

   使用消息中间件来解决。订阅 `topic` 然后生成当前服务器静态化的页面。

##### 1.2.2 数据更新

​	`FreeMarker` 生成的数据都是按模板生产好的。

- 如果修改了模板，就需要重新生成静态化文件，牵一发而动全身

- 如果后台数据有变更，如何即使同步到其它服务器？

#### 1.3 后台优化

##### 1.3.1 引入缓存

将商品详情加入 `redis` 缓存，这样第二次请求同一个商品，无需再查数据库，直接从` redis` 缓存中获取，**减少磁盘IO**。

######  redis 工具类

```java
public class RedisOpsUtil {

    @Autowired
    private RedisTemplate redisTemplate;

    public void set(String key,Object value){
        redisTemplate.opsForValue().set(key,value);
    }

    public void set(String key, Object value, long timeout, TimeUnit unit){
        redisTemplate.opsForValue().set(key,value,timeout,unit);
    }

    public boolean setIfAbsent(String key, Object value, long timeout, TimeUnit unit){
        return redisTemplate.opsForValue().setIfAbsent(key,value,timeout,unit);
    }

    public <T> T get(String key,Class<?> T){
        return (T)redisTemplate
                .opsForValue().get(key);
    }

    public String get(String key){
        return (String) redisTemplate
                .opsForValue().get(key);
    }

    public Long decr(String key){
        return redisTemplate
                .opsForValue().decrement(key);
    }

    public Long decr(String key,long delta){
        return redisTemplate
                .opsForValue().decrement(key,delta);
    }

    public Long incr(String key){
        return redisTemplate
                .opsForValue().increment(key);
    }

    public Long incr(String key,long delta){
        return redisTemplate
                .opsForValue().increment(key,delta);
    }

}
```

商品加入缓存

```java
// 请求的商品
...
    redsiOpsUtil.set(商品ID, 商品信息);
```

###### 缓存数据一致性

（1）**最终一致性方案**

设置超时时间来解决

```java
redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE+id,productInfo,360,T
imeUnit.SECONDS);
```

（2）**实时一致性方案**

**后续会讲到**（交易 canal binlog）

##### 1.3.2 问题

1. 提高请求的吞吐量，除了减少磁盘IO，还有网络IO。我们还可以发现，请求 redis 其实也会涉及到网络IO，我们所有的请求都需要走 6379 端口。

2. 高并发

   预期的 set 一次 redis，但实际是多次，并发问题。

   分布式锁：redis、zookeeper

3. 压缩（减少内存）

   序列化转成字符串

#### 1.4 加入分布式锁

```xml
1 <!‐‐加入redisson‐‐>
<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson</artifactId>
    <version>3.6.5</version>
</dependency>
```

**原理**：使用的 `setnx` 命令

```bash
SETNX key value
```

`setnx` 是 `SET if NOT EXISTS`（如果不存在，则 `SET`）的简写。

- 不存在	`SET` 成功，则返回 `int 1`
- 存在       `SET` 失败，则返回 `int 0`

```java
@Bean
public RedissonClient redissonClient() {
    Config config = new Config();
    config.useSingleServer()
       .setAddress("redis://tlshop.com:6379").setPassword("123456").setDatabase(0);
    return Redisson.create(config);
}
```

使用示例

（1）`lock()` 粗暴上锁，但所有的线程都会去获取锁，需使用双检锁

```java
// 获取缓存
...
RLock lock = redissonClient.getLock(lockPath + id);
try {
    lock.lock();
    // 调用本方法，再次获取缓存 (双重检测)
    ...
    // 业务代码
    ...
} finally {
    lock.unlock();
}
```

**优点**

- 简单粗暴
- 等待时间为 0

**缺点**

- 所有线程都需要**串行化**获取锁（与高并发环境违背）

- 使用**双检锁**，需要访问**两次** redis 缓存（**网络 IO**）

  

`tryLock()` 尝试获得锁，如果上锁失败那么就从缓存中取

```java
// 获取缓存
...
RLock lock = redissonClient.getLock(lockPath + id);
try {
    if(lock.tryLock(0, 5, SECONDS)){
        // 业务代码
    	...
    } else {
        Thread.sleep(50);
        // 递归调用本方法，获取缓存
        ...
    }
} finally {
    // 判断当前是否有锁（tryLock 可能失败，没有获取到锁，故可能释放锁失败）
    if (lock.isLocked()) {
        // 判断占有锁的是否为当前线程
        if (lock.isHeldByCurrentThread()) {
            lock.unlock();
        }
    }
}
```

**注意点**

`tryLock()`无参时，会存在默认的续命时间，线程会等待默认时间再次获取锁.

在此处，需要将续命时间设置为 `0`，同时将锁设置固定时间 `5` 秒，如果当前线程在 `5` 秒内未执行完，将由下一个线程执行。



**优点**

- 只需要获取**一次** `redis` 缓存（**网络 IO**）
- 线程不需要串行化获取锁

**缺点**

- 使用 **sleep** ，总会有一个线程存在**等待时间**



缓存引用场景

1. 访问量大、`QPS`高、更新频率不是很高的业务
2. 数据一致性要求不高

#### 1.5 缓存不足



##### 1.5.1 缓存击穿（热点数据单个key）

对于一些设置过期时间的 `key`，如果这些 `key` 可能再某些时间点被超高并发的访问，是一种非常“热点”的数据。此时，需要考虑缓存被“击穿”的问题。

与**缓存雪崩**的区别：

这里针对某一个 `key` 的缓存，雪崩是多个 `key`

![缓存击穿-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E5%AF%BC%E5%87%BA.png)



**解决方案**

1. **加锁**

   在未命中缓存时，通过加锁避免大量请求访问数据库。

1. **不允许过期**

   物理不过期，也就是不设置过期时间。而是逻辑上定时在后台异步的更新数据。

2. **采用二级缓存**

   `L1` 缓存失效时间短，`L2` 缓存失效时间长。请求优先从 `L1` 缓存获取数据，如果未命中，则加锁。保证只有一个线程去数据库中读取数据，然后再更新到 `L1` 和 `L2` 中。其它线程依然再 `L2` 缓存获取数据。

   

##### 1.5.2 缓存穿透（恶意攻击、访问不存在的数据）

缓存穿透是指查询一个一定不存在的数据，由于缓存时不命中时杯中写的，并且处于容错考虑，如果从存储层查询不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能 `DB` 就挂掉了。如果有人利用不存在的 `key` 频繁攻击我们的应用，这就是漏洞。

![缓存穿透-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E5%AF%BC%E5%87%BA.png)



**解决方案**

有很多种方法可以有效解决缓存穿透问题

1. 布隆过滤器

   最常用，将所有可能存在的数据哈希到一个足够大的 `BitMap` 中，一个一定不存在的数据会被 `BitMap` 拦截掉，从而避免了对底层存储系统的查询压力。

   ![布隆过滤器-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-%E5%AF%BC%E5%87%BA.png)

   

2. 缓存空数据

   简单粗暴。如果一个查询返回的数据为空（不管是数据不存在还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过 `5` 分钟。

   

##### 1.5.3 缓存雪崩（同一时间失效，并发量大）

在设置缓存时，采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 `DB`，`DB`瞬时压力过重雪崩。

![缓存雪崩-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9-%E5%AF%BC%E5%87%BA.png)



**解决方案**

1. 缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。

   这里分享一个简单方案就是：将缓存失效时间分散开。比如可以在原有的失效时间基础上增加一个随机值，比如`1~5分钟`随机。这样每个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

2. 事前

   这种方案就是在发生雪崩前对缓存集群实现高可用，如果是使用 `Redis`，可以使用 **主从 + 哨兵**，**Redis Cluster** 来避免 `Redis` 全盘崩溃的情况。

3. 事中

   使用 Hystrix 进行**限流 & 降级**。比如一秒来了 `5000` 个请求，可以设置假设只能有一秒 `2000` 个请求能通过这个组件，那么其他剩余的 3000 请求就会走限流逻辑。人啊后驱调用我们自己开发的降级组件（降级）。比如设置一些默认值等，以此来保护最后的 `MySQL` 不会被大量的请求给打死。

4. 事后

   开启 **Redis 持久化**机制，尽快恢复缓存集群



##### 1.5.4 缓存和数据库双写一致性

一致性问题是分布式常见问题，可以分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。



#### 1.6 本地缓存

网站的性能好与坏，**网络 IO** 和**磁盘 IO** 这两部分影响是比较大的。

我们现在引入缓存的目的就是提网站的性能。其实本质是不走磁盘走内存减少**磁盘 IO** 来提高性能。

但是引入 `redis` 后，同时也增加了**网络 IO**。那么就用到了本地缓存。



本地缓存不推荐 `ConcurrentHashMap`。因为它没有过期机制，需要自己实现。



#### 1.7 Guava 缓存

`pom` 依赖

```xml
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>22</version>
</dependency>
```

利用 `guava` 缓存作为**一级缓存**

```java
@Slf4j
@Component
public class LocalCache {

    private Cache<String,PmsProductParam> localCache = null;

    @PostConstruct
    private void init(){
        localCache = CacheBuilder.newBuilder()
                //设置本地缓存容器的初始容量
                .initialCapacity(10)
                //设置本地缓存的最大容量
                .maximumSize(500)
                //设置写缓存后多少秒过期
                .expireAfterWrite(60, TimeUnit.SECONDS).build();
    }


    public void setLocalCache(String key,PmsProductParam object){
        localCache.put(key,object);
    }

    public PmsProductParam get(String key){
       return localCache.getIfPresent(key);
    }
}
```

- 设置最大容量
- 初始化容量
- 缓存过期

两层缓存：本地缓存 + `redis` 缓存



#### 1.8 布隆过滤器

![布隆过滤器-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-%E5%AF%BC%E5%87%BA.png)

##### 1.8.1 存储

应用在加载时，将数据加入布隆过滤器中

```java
@Slf4j
@Configuration
public class BloomFilterConfig implements InitializingBean{

    @Autowired
    private PmsProductService productService;

    @Autowired
    private RedisTemplate template;

    @Bean
    public BloomFilterHelper<String> initBloomFilterHelper() {
        return new BloomFilterHelper<>((Funnel<String>) (from, into) -> into.putString(from, Charsets.UTF_8)
                .putString(from, Charsets.UTF_8), 1000000, 0.01);
    }

    /**
     * 布隆过滤器bean注入
     * @return
     */
    @Bean
    public BloomRedisService bloomRedisService(){
        BloomRedisService bloomRedisService = new BloomRedisService();
        bloomRedisService.setBloomFilterHelper(initBloomFilterHelper());
        bloomRedisService.setRedisTemplate(template);
        return bloomRedisService;
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        List<Long> list = productService.getAllProductId();
        log.info("加载产品到布隆过滤器当中,size:{}",list.size());
        if(!CollectionUtils.isEmpty(list)){
            list.stream().forEach(item->{
                //LocalBloomFilter.put(item);
                bloomRedisService().addByBloomFilter(RedisKeyPrefixConst.PRODUCT_REDIS_BLOOM_FILTER,item+"");
            });
        }
    }
}
```

##### 1.8.2 拦截匹配

```java
@Slf4j
public class BloomFilterInterceptor implements HandlerInterceptor {

    @Autowired
    private BloomRedisService bloomRedisService;

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        String currentUrl = request.getRequestURI();
        PathMatcher matcher = new AntPathMatcher();
        //解析出pathvariable
        Map<String, String> pathVariable = matcher.extractUriTemplateVariables("/pms/productInfo/{id}", currentUrl);
        //布隆过滤器存储在redis中
        if(bloomRedisService.includeByBloomFilter(RedisKeyPrefixConst.PRODUCT_REDIS_BLOOM_FILTER,pathVariable.get("id"))){
            return true;
        }

        /**
         * 存储在本地jvm布隆过滤器中
         */
        /*if(LocalBloomFilter.match(pathVariable.get("id"))){
            return true;
        }*/

        /*
         * 不在本地布隆过滤器当中，直接返回验证失败
         * 设置响应头
         */
        response.setHeader("Content-Type","application/json");
        response.setCharacterEncoding("UTF-8");
        String result = new ObjectMapper().writeValueAsString(CommonResult.validateFailed("产品不存在!"));
        response.getWriter().print(result);
        return false;

    }

}
```

##### 1.8.3 服务层

```java
public class BloomRedisService {

    private RedisTemplate<String, Object> redisTemplate;

    private BloomFilterHelper bloomFilterHelper;

    public void setBloomFilterHelper(BloomFilterHelper bloomFilterHelper) {
        this.bloomFilterHelper = bloomFilterHelper;
    }

    public void setRedisTemplate(RedisTemplate<String, Object> redisTemplate) {
        this.redisTemplate = redisTemplate;
    }

    /**
     * 根据给定的布隆过滤器添加值
     */
    public <T> void addByBloomFilter(String key, T value) {
        Preconditions.checkArgument(bloomFilterHelper != null, "bloomFilterHelper不能为空");
        int[] offset = bloomFilterHelper.murmurHashOffset(value);
        for (int i : offset) {
            redisTemplate.opsForValue().setBit(key, i, true);
        }
    }

    /**
     * 根据给定的布隆过滤器判断值是否存在
     */
    public <T> boolean includeByBloomFilter(String key, T value) {
        Preconditions.checkArgument(bloomFilterHelper != null, "bloomFilterHelper不能为空");
        int[] offset = bloomFilterHelper.murmurHashOffset(value);
        for (int i : offset) {
            if (!redisTemplate.opsForValue().getBit(key, i)) {
                return false;
            }
        }
        return true;
    }
}
```

##### 1.8.4 工具类

```java
/**
 * 算法过程：
 * 1. 首先需要k个hash函数，每个函数可以把key散列成为1个整数
 * 2. 初始化时，需要一个长度为n比特的数组，每个比特位初始化为0
 * 3. 某个key加入集合时，用k个hash函数计算出k个散列值，并把数组中对应的比特位置为1
 * 4. 判断某个key是否在集合时，用k个hash函数计算出k个散列值，并查询数组中对应的比特位，如果所有的比特位都是1，认为在集合中。
 **/
public class BloomFilterHelper<T> {
    private int numHashFunctions;

    private int bitSize;

    private Funnel<T> funnel;

    public BloomFilterHelper(Funnel<T> funnel, int expectedInsertions, double fpp) {
        Preconditions.checkArgument(funnel != null, "funnel不能为空");
        this.funnel = funnel;
        // 计算bit数组长度
        bitSize = optimalNumOfBits(expectedInsertions, fpp);
        // 计算hash方法执行次数
        numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, bitSize);
    }

    public int[] murmurHashOffset(T value) {
        int[] offset = new int[numHashFunctions];

        long hash64 = Hashing.murmur3_128().hashObject(value, funnel).asLong();
        int hash1 = (int) hash64;
        int hash2 = (int) (hash64 >>> 32);
        for (int i = 1; i <= numHashFunctions; i++) {
            int nextHash = hash1 + i * hash2;
            if (nextHash < 0) {
                nextHash = ~nextHash;
            }
            offset[i - 1] = nextHash % bitSize;
        }

        return offset;
    }

    /**
     * 计算bit数组长度
     */
    private int optimalNumOfBits(long n, double p) {
        if (p == 0) {
            // 设定最小期望长度
            p = Double.MIN_VALUE;
        }
        return (int) (-n * Math.log(p) / (Math.log(2) * Math.log(2)));
    }

    /**
     * 计算hash方法执行次数
     */
    private int optimalNumOfHashFunctions(long n, long m) {
        return Math.max(1, (int) Math.round((double) m / n * Math.log(2)));
    }
}
```

### 2. 亿级流量商品架构

![亿级流量架构-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E4%BA%BF%E7%BA%A7%E6%B5%81%E9%87%8F%E6%9E%B6%E6%9E%84-%E5%AF%BC%E5%87%BA.png)

#### 2.1 动态化渲染

`OpenResty®` 是一个基于 `Nginx` 与 `Lua` 的高性能 `Web` 平台，其内部继承了大量精良的 `Lua` 库、第三方模块以及大多数的依赖项。用于方便搭建能够处理超高并发、扩展性极高的动态 `Web` 应用、`Web` 服务和动态网关。

`Web` 开发人员和系统工程师可以使用 `Lua` 脚本语言调动 `Nginx` 支持的各种 `C` 以及 `Lua` 模块，快速构造出足以胜任 `10K` 乃至 `1000K` 以上单机并发连接的高性能 Web 应用系统。`OpenResty®` 的目标是让你的`Web`服务直接跑在 `Nginx` 服务内部，充分利用 `Nginx` 的非阻塞 `I/O` 模型，不仅仅对 `HTTP` 客户端请求,甚至于对远程后端诸如`MySQL`、`PostgreSQL`、`Memcached` 以及 `Redis` 等都进行一致的高性能响应。

`OpenResty®`下载地址：http://openresty.org/cn/download.html



##### 2.1.1 流量分发

![流量分发-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E6%B5%81%E9%87%8F%E5%88%86%E5%8F%91-%E5%AF%BC%E5%87%BA.png)



###### 引入`OpenResty HTTP` 模块

流量分发的 `Nginx` 会发送 HTTP 请求到后端的应用层 `Nginx`，故需要先引用 `Lua http` 模块

下载地址：https://github.com/ledgetech/lua-resty-http/tree/master/lib/resty

- http.lua
- http_headers.lua

 [lua-resty-http-master.zip](software\lua-resty-http-master.zip) 

文件放置 `I:\openresty\openresty\lualib\resty\` 下。

在 `nginx.conf` 的 `http` 模块中加入支持 `Lua` 相关的包：

```powershell
lua_package_path '../lualib/?.lua;;';
lua_package_cpath '../lualib/?.so;;';
include lua.conf;
```

`lua.conf `如下：（其实就是将`nginx.conf`中`http`模块的`server`单独提出来）

```shell
server {
    listen 300;
    location /product {
        default_type 'text/html;charset=UTF‐8';
        lua_code_cache on;
        content_by_lua_file D:/ProgramData/nginx/dis.lua;
    }
}
```

监听 `300` 端口，如果请求路径是 `product`，那么就让它包含 `distribution.lua` 文件的内容，并开启 `lua` 缓存。

`distribution.conf`如下：

```lua
local uri_args = ngx.req.get_uri_args()
local productId = uri_args["productId"]

-- 流量分发接收服务器
local host = {"127.0.0.1:222","127.0.0.1:333"}
local hash = ngx.crc32_long(productId)
hash = (hash % 2) + 1
backend = "http://"..host[hash]
local method = uri_args["method"]
local requestBody = "/"..method.."?productId="..productId
local http = require("resty.http")
local httpc = http.new()

local resp, err = httpc:request_uri(backend, {
	method = "GET",
	path = requestBody,
	keepalive=false
})

if not resp then
	ngx.say("request error :", err)
	return
end

ngx.say(resp.body)

httpc:close()
```

截取传过来的 `productId`，然后从配置的 `host`服务中 `hash` 去取其中一台服务，再通过服务请求拿到相应的数据，并对数据进行输出。



`222` 和 `333` 的配置

`lua.conf`如下：

```shell
lua_shared_dict my_cache 128m;
server {
    listen 222;
    set $template_location "/templates";
    set $template_root "I:/openresty/";

    location /product {
    default_type 'text/html;charset=UTF‐8';
    lua_code_cache on;
    content_by_lua_file I:/openresty/product.lua;
    }
}
```

##### 2.1.2 请求后台

###### 引入`OpenResty Template`模块

模板渲染使用是 `OpenResty Template`模块

下载地址：https://opm.openresty.org/package/bungle/lua-resty-template/

- /template/
  - html.lua
  - microbenchmark.lua
  - safe.lua
- template.lua



###### 加入缓存

`product.lua`如下：

```lua
-- 获取请求路径参数数组
local uri_args = ngx.req.get_uri_args()
-- 获取商品 ID
local productId = uri_args["productId"]
-- 定义 nginx 缓存
local cache_ngx = ngx.shared.my_cache
-- 拼接缓存 key
local productCacheKey = "product_info_"..productId
-- 从缓存中获取 key 对应的商品信息
local productCache = cache_ngx:get(productCacheKey)
-- 缓存未命中，请求商品服务
if productCache == "" or productCache == nil then
	local http = require("resty.http")
	local httpc = http.new()
	local resp, err = httpc:request_uri("http://127.0.0.1:8866",{
		method = "GET",
		path = "/pms/productInfo/"..productId
	})
	productCache = resp.body
	local expireTime = math.random(600,1200)
	cache_ngx:set(productCacheKey, productCache, expireTime)
end

local cjson = require("cjson")
-- 反序列化，获得商品 Json 字符串
local productCacheJSON =cjson.decode(productCache)
ngx.say(productCache);
-- 解析商品信息
local context = {
    id = productCacheJSON.data.id,
    name = productCacheJSON.data.name,
    price = productCacheJSON.data.price,
    pic = productCacheJSON.data.pic,
    detailHtml = productCacheJSON.data.detailHtml
}
-- 获取模板文件
local template = require("resty.template")
-- 将 context 映射到 template 文件中
template.render("product.html", context)
```

###### `HTML` 模板

语法如下：（`{* id *}`）

```html
<html>
<head>
<meta http‐equiv="Content‐Type" content="text/html; charset=utf‐8" />
</head>
<body>
	<h1>
	商品id: {* id *}<br/>
	商品名称: {* name *}<br/>
	商品价格: {* price *}<br/>
	商品库存: <img src={* pic *}/><br/>
	商品描述: {* detailHtml *}<br/>
	</h1>
</body>
</html>
```

 [product.html](software\product.html) 



###### 关闭 `nginx` 命令

```cmd
taskkill /im nginx.exe /f
```

##### 2.1.3 渲染图

模板动态渲染图如下：

![模板动态渲染](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E6%A8%A1%E6%9D%BF%E5%8A%A8%E6%80%81%E6%B8%B2%E6%9F%93.png)

 

#### 2.2 多级缓存

![多级缓存-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98-%E5%AF%BC%E5%87%BA.png)



##### 2.2.1 一级缓存

实现方式：`Lua` + `Nginx`

特点：数据量小、访问量相对很高



##### 2.2.2 二级缓存

实现方式：`JVM`本地缓存（`GUAVA`）

特点：数据量很大、访问量相对高



##### 2.2.3 三级缓存

实现方式：`Redis`

特点：数据量相对比较大、访问量相对不高



##### 2.2.4 热度问题

如何保证最热的数据在最前面 ？

- 设置各级缓存的内存大小，越靠前的缓存其容量越小

- 设置各级缓存的过期时间，越靠前的缓存其过期时间越小
- 使用 `LRU-K`(链表 `K`) 最近最热的一次数据连续访问 `3` 次以上，放入一级缓存



### 3. 微服务拆分

![image-20230115213350295](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/image-20230115213350295.png)

原图地址：https://www.processon.com/view/link/5e69e768e4b07fc7a6841488



#### 3.1 微服务拆分时机

如下场景需要进行微服务拆分：

- 代码维护困难，几百人同时开发一个模块，提交代码频繁出现大量冲突
- 模块耦合严重，相互依赖，小工嗯呢该修改也必须累计到大版本才能上线
- 很想扩展流程复杂，主要业务和次要业务耦合。例如：下单和支付业务需要扩容，而注册业务不需要要扩容

微服务不仅仅是技术的升级，更是开发方式、组织架构、开发观念的转变。



#### 3.2 微服务拆分的一些通用原则

**单一服务内部功能高内聚低耦合**：每个服务只完成自己职责内的任务，对于不是自己职责的功能交给其它服务来完成

**闭包原则（CCP）：**微服务的闭包原则就是当我们需要改变一个微服务的时候，所有依赖都在这个微服务的组件内，不需要修改其他微服务

**服务自治、接口隔离原则：**尽量消除对其他服务的强依赖，这样可以降低沟通成本，提升服务稳定性。服务通过标准的

接口隔离，隐藏内部实现细节。这使得服务可以独立开发、测试、部署、运行，以服务为单位持续交付。

**持续演进原则：**在服务拆分的初期，你其实很难确定服务究竟要拆成什么样。应逐步划分，持续演进，避免服务数量的爆炸性增长。

**拆分的过程尽量避免影响产品的日常功能迭代：**也就是说要一边做产品功能迭代，一边完成服务化拆分。比如优先剥离比较独立的边界服务（如短信服务等），从非核心的服务出发减少拆分对现有业务的影响，也给团队一个练习、试错的机会。同时当两个服务存在依赖关系时优先拆分被依赖的服务。

**服务接口的定义要具备可扩展性：**比如微服务的接口因为升级把之前的三个参数改成了四个，上线后导致调用方大量报错，推荐做法服务接口的参数类型最好是封装类，这样如果增加参数就不必变更接口的签名**避免环形依赖与双向依赖：**尽量不要有服务之间的环形依赖或双向依赖，原因是存在这种情况说明我们的功能边界没有化分清楚或者有通用的功能没有下沉下来。

**阶段性合并：**随着你对业务领域理解的逐渐深入或者业务本身逻辑发生了比较大的变化，亦或者之前的拆分没有考虑的很清楚，导致拆分后的服务边界变得越来越混乱，这时就要重新梳理领域边界，不断纠正拆分的合理性。

**自动化驱动：**部署和运维的成本会随着服务的增多呈指数级增长，每个服务都需要部署、监控、日志分析等运维工作，成本会显著提升。因此，在服务划分之前，应该首先构建自动化的工具及环境。开发人员应该以自动化为驱动力，简化服务在创建、开发、测试、部署、运维上的重复性工作，通过工具实现更可靠的操作，避免微服务数量增多带来的开发、管理复杂度问题。

**拆分粒度控制**

思考： 拆分的粒度是不是越细越好？

目前很多传统的单体应用再向微服务架构进行升级改造，如果拆分粒度太细会增加运维复杂度，粒度过大又起不到效果，那么改造过程中如何平衡拆分粒度呢？平衡拆分粒度可以从两方面进行权衡，一是业务发展的复度，二是团队规模的人数人员和服务数量的不匹配，会导致维护成本增加，也会导致服务合并。

前期设计和开发阶段： 3个人负责一个微服务后期维护阶段：每个微服务可以安排2个人维护，每个人可以维护多个微服务

**功能维度拆分策略**

大的原则是基于业务复杂度拆分服务： 业务复杂度足够高，应该基于领域驱动拆分服务。业务复杂度较低，选择基于数据驱动拆分服务

基于数据驱动拆分服务： 自下而上的架构设计方法，通过分析需求，确定整体数据结构，根据表之间的关系拆分服务。

拆分步骤： 需求分析，抽象数据结构，划分服务，确定调用关系和业务流程验证。

基于领域驱动拆分服务： 自上而下的架构设计方法，通过和领域专家建立统一的语言，不断交流，确定关键业务场景，逐步确定边界上下文。领域驱动更强调业务实现效果，认为自下而上的设计可能会导致技术人员不能更好地理解业务方向，进而偏离业务目标。

拆分步骤：通过模型和领域专家建立统一语言，业务分析，寻找聚合，确定服务调用关系，业务流程验证和持续优化。

以电商的场景为例，交易链路划分的限界上下文如下图左半部分，根据一个限界上下文可以设计一个微服务，拆解出来的微服务如下图右侧部分。还有一种常见拆分场景，从已有单体架构中逐步拆分服务。

拆分步骤： 前后端分离，提取公共基础服务（如单点登录），不断从老系统抽取服务，垂直划分优先，适当水平切分以上几种拆分方式不是多选一，而是可以根据实际情况自由排列组合。**同时拆分不仅仅是架构上的调整，也意味着要在组织结构上做出相应的适应性优化，以确保拆分后的服务由相对独立的团队负责维护。**

**非功能维度拆分策略**

主要考虑六点包括扩展性、复用性、高性能、高可用、安全性、异构性

**扩展性**

区分系统中变与不变的部分，不变的部分一般是成熟的、通用的服务功能，变的部分一般是改动比较多、满足业务迭代扩展性需要的功能，我们可以将不变的部分拆分出来，作为共用的服务，将变的部分独立出来满足个性化扩展需要同时根据二八原则，系统中经常变动的部分大约只占 20%，而剩下的 80% 基本不变或极少变化，这样的拆分也解决了发布频率过多而影响成熟服务稳定性的问题。

**复用性**

不同的业务里或服务里经常会出现重复的功能，比如每个服务都有鉴权、限流、安全及日志监控等功能，可以将这些通过的功能拆分出来形成独立的服务，也就是微服务里面的 API 网关。

**高性能**

将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其它服务。常见的拆分方式和具体的性能瓶颈有关，例如电商的抢购，性能压力最大的是入口的排队功能，可以将排队功能独立为一个服务。我们也可以基于读写分离来拆分，比如电商的商品信息，在 App 端主要是商详有大量的读取操作，但是写入端商家中心访问量确很少。因此可以对流量较大或较为核心的服务做读写分离，拆分为两个服务发布，一个负责读，另外一个负责写。

数据一致性是另一个基于性能维度拆分需要考虑的点，对于强一致的数据，属于强耦合，尽量放在同一个服务中（但是有时会因为各种原因需要进行拆分，那就需要有响应的机制进行保证），弱一致性通常可以拆分为不同的服务。

**高可用**

将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，然后重点保证核心服务的高可用。具体拆分的时候，核心服务可以是一个也可以是多个，只要最终的服务数量满足“三个火枪手”的原则就可以。

**安全性**

不同的服务可能对信息安全有不同的要求，因此把需要高度安全的服务拆分出来，进行区别部署，比如设置特定的 DMZ区域对服务进行分区部署，可以更有针对性地满足信息安全的要求，也可以降低对防火墙等安全设备吞吐量、并发性等方面的要求，降低成本，提高效率。

**异构性**

对于对开发语言种类有要求的业务场景，可以用不同的语言将其功能独立出来实现一个独立服务。**拆分注意的风险**

**不打无准备之仗：**开发团队是否具备足够的经验，能否驾驭微服务的技术栈，可能是第一个需要考虑的点。

**不断纠正：**我们需要承认我们的认知是有限的，只能基于目前的业务状态和有限的对未来的预测来制定出一个相对合适

的拆分方案，而不是所谓的最优方案，任何方案都只能保证在当下提供了相对合适的粒度和划分原则，要时刻做好在未来的末一个时刻会变得不和时宜、需要再次调整的准备。

**要做行动派，而不是理论派：**在具体怎么拆分上，也不要太纠结于是否合适，如果拆了之后发现真的不合适，在重新调整就好了。如果要灵活调整，可以针对服务化架构搭建起一套完成的能力体系，比如服务治理平台、数据迁移工具、数据双写等等

**服务只拆不合：**

拆相当于我们开发代码，合相当于重构代码。随着我们对应用程序领域的了解越来越深，它们需要随着时间的推移而变化。

人员和服务数量的不匹配，导致的维护成本增加，也是导致服务合并的一个重要原因。

如果微服务数量过多和资源不匹配，则可以考虑合并多个微服务到服务包，部署到一台服务器，这样可以节省服务运行时的基础资源消耗也降低了维护成本。需要注意的是，虽然服务包是运行在一个进程中，但是服务包内的服务依然要满足微服务定义，以便在未来某一天要重新拆开的时候可以很快就分离

#### 3.3 Spring Cloud 技术栈选型

`Spring Cloud Alibaba`官网：https://github.com/alibaba/spring-cloud-alibaba/wiki

SpringCloud 的几大痛点：

- SpringCloud 部分组件停止维护和更新，给开发带来不便;

- SpringCloud 部分环境搭建复杂，没有完善的可视化界面，我们需要大量的二次开发和定制

- SpringCloud 配置复杂，难以上手，部分配置差别难以区分和合理应用

SpringCloud Alibaba 的优势:

- 阿里使用过的组件经历了考验，性能强悍，设计合理，现在开源出来大家用成套的产品搭配完善的可视化界面给开发运维带来极大的便利

- 搭建简单，学习曲线低。

所以我们优先选择 Spring Cloud Alibaba 提供的微服务组件

Spring Cloud Alibaba 官方推荐版本选择：

| Spring Cloud Alibaba Version | Spring Cloud Version  | Spring Boot Version |
| ---------------------------- | --------------------- | ------------------- |
| 2022.0.0.0-RC*               | Spring Cloud 2022.0.0 | 3.0.0               |

| Spring Cloud Alibaba Version | Spring Cloud Version  | Spring Boot Version |
| ---------------------------- | --------------------- | ------------------- |
| 2021.0.4.0*                  | Spring Cloud 2021.0.4 | 2.6.11              |
| 2021.0.1.0                   | Spring Cloud 2021.0.1 | 2.6.3               |
| 2021.1                       | Spring Cloud 2020.0.1 | 2.4.2               |

pom 引入：

```xml
<parent>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring‐boot‐starter‐parent</artifactId>
	<version>2.1.7.RELEASE</version>
	<relativePath/> <!‐‐ lookup parent from repository ‐‐>
</parent>
<groupId>com.tuling</groupId>
<artifactId>tuling‐mall</artifactId>
<version>0.0.1‐SNAPSHOT</version>
<name>tuling‐mall</name>
<packaging>pom</packaging>

<properties>
	<spring‐cloud.version>Greenwich.SR3</spring‐cloud.version>
	<spring‐cloud‐alibaba.version>2.1.2.RELEASE</spring‐cloud‐alibaba.version>
</properties>

<dependencyManagement>
	<dependencies>
		<!‐‐Spring Cloud 相关依赖‐‐>
		<dependency>
			<groupId>org.springframework.cloud</groupId>
			<artifactId>spring‐cloud‐dependencies</artifactId>
			<version>${spring‐cloud.version}</version>
			<type>pom</type>
			<scope>import</scope>
		</dependency>
		<!‐‐Spring Cloud Alibaba 相关依赖‐‐>
		<dependency>
			<groupId>com.alibaba.cloud</groupId>
			<artifactId>spring‐cloud‐alibaba‐dependencies</artifactId>
			<version>${spring‐cloud‐alibaba.version}</version>
			<type>pom</type>
			<scope>import</scope>
		</dependency>
	</dependencies>
</dependencyManagement>
```

#### 3.4 Nacos 服务高可用搭建**（待完善）**

##### 3.4.1 Nacos UI

访问地址：http://192.168.10.148:8848/nacos/

用户名：nacos	密码：nacos

![nacos-ui](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/nacos-ui.png)

#### 3.5 服务发现

将微服务注册到 `Nacos` 服务器

##### 3.5.1 引入依赖

需要注册的服务引入依赖，服务启动时自动注册到 `Nacos` 服务中心

```xml
<!-- 注册中心 服务注册与发现 -->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>
```

**注**：**@EnableDiscoveryClient** `Spring Cloud` 已经实现了该注解的功能，无需在主程序上添加该注解。

##### 3.5.2 在 yml 中配置注册中心地址

```yml
spring:
  application:
    name: tulingmall-member	# 微服务名称
  cloud:
    nacos:
      discovery:
        server-addr: tl.nacos.com:8848	# Nacos 服务器地址
        namespace: 80a98d11‐492c‐4008‐85aa‐32d889e9b0d0	 # 环境隔离
```

##### 3.5.3 环境隔离

服务使用命名空间进行分组。

`namespace` 为 `Nacos UI` 页面创建的**命名空间**的 `ID` ：

![分组-命名空间](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%88%86%E7%BB%84-%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4.png)



启动该服务，在 `Nacos UI` 上查看，该服务已存在对应的分组下：

![服务分组](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E6%9C%8D%E5%8A%A1%E5%88%86%E7%BB%84.png)



#### 3.6 RPC 调用

##### 3.6.1 引入依赖

`RPC` 使用的是 `Feign`，引入 `Feign` 的依赖：

```xml
<!--服务远程调用 -->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
```

##### 3.6.2 添加`@EnableFeignClients`注解

在消费端启动类上添加 `@EnableFeignClients`注解，开启 `OpenFeign` 远程调用功能：

```java
@SpringBootApplication
@EnableFeignClients
public class TulingmallMemberApplication {
    public static void main(String[] args) {
    	SpringApplication.run(TulingmallMemberApplication.class, args);
    }
}
```

##### 3.6.3 编写调用接口**（疑问）**

在消费端编写调用接口

```java
/**
 * 调用远程服务
 *
 * name 为远程要调用的 服务名
 * path 为远程要调用的服务前缀
 * url  为开发时用来写死访问地址（ip:port），用于测试（不会走负载均衡，直接调用对应节点）
 */
@FeignClient(name = "tulingmall‐coupons",path = "/coupon")
public interface CouponsFeignService {

    @RequestMapping(value = "/list", method = RequestMethod.GET)
	@ResponseBody
	CommonResult<List<SmsCouponHistory>> list(
        @RequestParam(value = "useStatus", required = false) Integer useStatus,
        @RequestHeader("memberId") Long memberId);
}
```

**疑问：`@FengiClient`注解中 path 的作用 ？**



##### 3.6.4 发起远程调用

```java
// 注入远程服务
@Autowired
private CouponsFeignService couponsFeignService;

@RequestMapping(value = "/coupons", method = RequestMethod.GET)
public CommonResult getCoupons(
    @RequestParam(value = "useStatus", required = false) Integer useStatus,
    @RequestHeader("memberId") Long memberId){
    
	// 通过openfeign从远程微服务tulingmall‐coupons获取优惠券信息
	return couponsFeignService.list(useStatus, memberId);
}
```

##### 3.6.5 OpenFeign 日志配置**（待完善）**

开启 OpenFeign 日志配置

```java
@Configuration
public class FeignConfig {
    
    @Bean
    public Logger.Level feignLoggerLevel() {
        return Logger.Level.FULL;
    }
}
```

如果日志不显示，可以在 yml 中通过 logging.level 设置日志级别

```yaml
logging:
	level:
	com.tuling: debug
```

微服务拆分，可将 `Feign` 独立拆分出去。



#### 3.7 配置中心

![配置中心-导出](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83-%E5%AF%BC%E5%87%BA.png)



##### 3.7.1 bootstrap.yml 和 application.yml

`bootstrap.yml` 文件也是 `Springboot` 的默认配置文件，而且其加载的时间比`application.yml `更早。

`application.yml` 和 `bootstrap.yml` 虽然都是 `Springboot` 的默认配置文件，但定位却不相同。

- `bootstrap.yml`	可以理解成系统级别的一些参数配置，这些 参数一般是不会变动的。
- `application.yml`    可以用来定义应用级别的参数，如果搭配 `spring cloud config` 使用，`application.yml` 里边定义的问价你可以实现动态替换。

**总结**：

​		`bootstrap.yml` 文件相当于项目启动时的引导文件，内容相对固定。`application.yml` 文件是微服务的一些常规配置参数，变化比较频繁。`bootstrap.yml` 先于 `application.yml`。



编写 `bootstrap.yml` 配置文件，该文件中的配置无需随着环境而变更：

 ```yml
spring:
	application:
		name: tulingmall‐member #微服务的名称
	cloud:
        nacos:
            config:
                serverAddr: 192.168.65.232:8848 #配置中心的地址
                namespace: 741b4a7b‐c610‐4f88‐8b83‐e9ec87e68319

                # dataid 为 yml 的文件扩展名配置方式
                # `${spring.application.name}.${file‐extension:properties}`
                file‐extension: yml

                # 通用配置(拉取通用配置的ID)
                shared‐dataids: nacos.yml,mybatis.yml,actuator.yml,redis.yml
                # 刷新配置(动态改配置可以刷新)
                refreshable‐dataids: nacos.yml,mybatis.yml,actuator.yml,redis.yml

# profile 粒度的配置
#`${spring.application.name}‐${profile}.${file‐extension:properties}`
profiles:
	active: dev
 ```

##### 3.7.2 抽取公用配置

在 `Nacos UI` 界面，新增 `nacos.yml` 公共配置

```yaml
spring:
  cloud:
    nacos:
      discovery:
        server-addr: tl.nacos.com:8848
```

![抽取公共配置nacos](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E6%8A%BD%E5%8F%96%E5%85%AC%E5%85%B1%E9%85%8D%E7%BD%AEnacos.png)

##### 3.7.3 定义 dev 应用配置

在 `nacos UI` 界面，`dev` 命名空间下，定义` { `服务名 + dev` }.yml` 的配置文件

![应用配置-dev](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/%E5%BA%94%E7%94%A8%E9%85%8D%E7%BD%AE-dev.png)



#### 3.8 服务网关 Gateway

定义一个新的 module，为 服务网关 gateway 模块



##### 3.8.1 引入父 pom

```xml
<parent>
    <groupId>com.tuling</groupId>
    <artifactId>tuling-mall</artifactId>
    <version>0.0.1-SNAPSHOT</version>
</parent>
```

##### 3.8.2 引入网关依赖

```xml
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-gateway</artifactId>
</dependency>
```

##### 3.8.3 服务注册

服务经过网关之后，需要找到下一个微服务，所以网关需要在 Nacos 上注册，以发现其它微服务

```xml
<!-- Nacos 服务注册与发现 -->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
</dependency>

<!-- Nacos 配置中心 -->
<dependency>
    <groupId>com.alibaba.cloud</groupId>
    <artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
</dependency>

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
```

**注意**：会和 spring-webmvc 的依赖冲突，需要排除 spring-webmvc



编写应用配置文件 application.yml

```yaml
server:
  port: 8888
spring:
  application:
    name: tulingmall-gateway
  cloud:
    nacos:
      discovery:
        server-addr: tl.nacos.com:8848
        namespace: 4a08c9666f5301bb648463f6d11a12f6 	# 环境隔离
        
    gateway:
      discovery:
        locator:
          # 默认为 false，true 表示开启通过微服务创建路由的功能，即可以通过微服务名访问服务
          lower-case-service-id: true
          # 是否开启网关
          enabled: true
          
	  # 下一个服务路由
      routes:
        # 一个 ID 对应一个微服务，路由 ID，全局唯一
        - id: tulingmall-authcenter
        # 底层有全归过滤器，用来做负载均衡，从微服务里拉取对应微服务，再取出一个节点往下一个传递
          uri: lb://tulingmall-authcenter
          # 断言，有个断言工厂，用来匹配路径
          predicates:
            # 当前访问路径里有 oauth 时，会来找这个微服务
            - Path=/oauth/**
        - id: tulingmall-member
          uri: lb://tulingmall-member
          predicates:
            - Path=/sso/**,/member/**
        - id: tulingmall-coupons
          uri: lb://tulingmall-coupons
          predicates:
            - Path=/coupon/**
            
logging:
  level:
    org.springframework.cloud.gateway: debug
```

#### 3.9 接入 Skywalking

##### 3.9.1 搭建 `Skywalking OAP` 服务

![Skywalking-oap](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/Skywalking-oap.png)

##### 3.9.2 微服务配置 Skywalking Agent

使用 JVM 参数的方式启动微服务

```shell
-javaagent:G:\tuling\skywalking-agent\skywalking-agent.jar
-Dskywalking.agent.service_name=tulingmall-member
-Dskywalking.collector.backend_service=192.168.10.153:11800
```

![jvm参数方式配置skywalking-agent](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/jvm%E5%8F%82%E6%95%B0%E6%96%B9%E5%BC%8F%E9%85%8D%E7%BD%AEskywalking-agent.png)



启动微服务，在 Skywalking UI 上可以查看对应服务

![Skywalking 服务查看](https://butterfly-1316798368.cos.ap-nanjing.myqcloud.com/images/Skywalking%20%E6%9C%8D%E5%8A%A1%E6%9F%A5%E7%9C%8B.png)

